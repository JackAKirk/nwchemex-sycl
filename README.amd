Instructions for building on crusher:

SYCL:
Some module customization is neccessary:

source setup

In this build we don't use Cray-aware MPI, so
export MPICH_GPU_SUPPORT_ENABLED=0

Note that this assumes there is a built-from-source llvm in $HOME/tools/llvm-build/install. This is built with
rocm 5.5.1 loaded. Configuration is as follows:

python3 llvm/buildbot/configure.py --hip -o llvm-build --cmake-opt=-DSYCL_BUILD_PI_HIP_ROCM_DIR=$ROCM_PATH
python3 llvm/buildbot/compile.py -o llvm-build

Then:
bash build-mkl.sh
# add oneMKL to LD_LIBRARY_PATH
export LD_LIBRARY_PATH=$PWD/oneMKL/install/lib:$LD_LIBRARY_PATH
# build tamm
bash build-nwchem-cuda-tamm.sh
# build CC
build-nwchem-cuda-CC.sh

On crusher the executable and the input file need to be copied to a writeable filesystem, e.g., somewhere on /lustre.

Exachem_sycl_cuda/install_sycl_cuda/bin contains CCSD_T . Copy Exachem_sycl_cuda/CoupledCluster/inputs/uracil.json
to the bin directory and edit it:

$ diff uracil.json ../../CoupledCluster/inputs/
41c41
<       "skip_ccsd": true,
---
>       "skip_ccsd": false,

Then

bash srun.sh N

where N is the number of GPUs to use. Note that srun.sh uses run.sh to set affinity to a particulary GPU based on MPI rank. There may be better ways to do this.

This has been tested only on a single node.


HIP:
source setup.hip
bash build-nwchem-amd-tamm-hip.sh
bash build-nwchem-amd-CC-hip.sh

Run instructions are about the same.


Running the big problem:
sub.sh or sub-hip.sh will queue batch jobs for the problem in the paper
"Towards Cross-Platform Portability of Coupled-Cluster Methods with Perturbative Triples using SYCL"

Modifications will be required for the correct account and location of the setup script and executable.
